{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrZEU5lbOec5"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1EOzaynFEkpMjE-PWPqllAYWRScW-2YQ0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBF-98aMOi4F"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "\n",
        "<h1> Avisek Gupta </h1>\n",
        "\n",
        "<h1> Indian Statistical Institute, Kolkata </h1>\n",
        "\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75sa0VRy9WHD"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "\n",
        "<h1> Tutorial 3: Training Neural Networks </h1>\n",
        "\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tfwNNiDO4pC"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 1. <b>torch.optim</b> updates the parameters based on the computed gradients.</h2>\n",
        " \n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OT_wAPWOOjpL",
        "outputId": "1476b3cd-97a7-4f54-9f11-9d912cfee8b7"
      },
      "source": [
        "# Gradient Descent using torch.optim\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "x = torch.randn(1, requires_grad=True, device=device)\n",
        "print('Initial x:', x)\n",
        "\n",
        "max_iter = 100\n",
        "learning_rate = 0.1\n",
        "eps = 1e-6\n",
        "\n",
        "# (a) Initialize Optimizer\n",
        "optimizer = optim.SGD([x], lr=learning_rate)\n",
        "\n",
        "for _ in range(max_iter):\n",
        "    optimizer.zero_grad()  # (b) i. Explicitly set the gradients to zero\n",
        "    y = (x - 4) ** 2\n",
        "    y.backward()  # (b) ii. Compute the gradients\n",
        "    prev_x = x.clone()\n",
        "    optimizer.step()  # (b) iii. Update the parameters using the computed gradients\n",
        "    if torch.norm(prev_x - x) < eps:\n",
        "        break\n",
        "\n",
        "print('Soln x:', x)\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial x: tensor([0.1263], requires_grad=True)\n",
            "Soln x: tensor([4.0000], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8FHLvAZRbf_"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 2. Steps of using an optimizer: </h2>\n",
        "\n",
        "<h2> (a) Initialize Optimizer </h2>\n",
        "\n",
        "<h2> (b) In the Optimization loop:\n",
        "\n",
        "<h2> i. Explicitly set the gradients to zero </h2>\n",
        "\n",
        "<h2> ii. Compute the gradients </h2>\n",
        "\n",
        "<h2> iii. Update the parameters using the computed gradients </h2>\n",
        " \n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhOjEIpMTW0c"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 3. (i) Implementing a simple one-hidden layer Multi-Layered Perceptron </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvnHjI-9TVZ9",
        "outputId": "68328636-d42e-4211-94d4-f0ec424950c8"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load in the digits data set from scikit-learn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "X = load_digits().data\n",
        "X = X / X.max()\n",
        "y = LabelBinarizer().fit_transform(load_digits().target) # Convert each label to one-hot vectors\n",
        "# Split the data into training and test data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
        "# Convert numpy arrays to torch tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "# Multi-Layered Perceptron Architecture\n",
        "# Input dim: X.shape[1]\n",
        "# Hidden Layer dim: 200\n",
        "# Output Layer dim: n_classes\n",
        "input_dim = X.shape[1]\n",
        "hidden_layer_size = 200\n",
        "n_classes = y.shape[1]\n",
        "\n",
        "# Initialize network parameters\n",
        "W1 = torch.rand(input_dim+1, hidden_layer_size) * 2 - 1\n",
        "W2 = torch.rand(hidden_layer_size+1, n_classes) * 2 - 1\n",
        "\n",
        "# Push everything on to the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "W1 = W1.to(device)\n",
        "W2 = W2.to(device)\n",
        "W1.requires_grad = True\n",
        "W2.requires_grad = True\n",
        "const_ones = torch.ones(X_train.shape[0], 1).to(device)\n",
        "const_ones.requires_grad = False\n",
        "\n",
        "# Training parameters\n",
        "max_epochs = 10000\n",
        "learning_rate = 1e-4\n",
        "\n",
        "optimizer = optim.SGD([W1, W2], lr=learning_rate)  # Initialize Optimizer\n",
        "\n",
        "for n_epoch in range(max_epochs):\n",
        "    cost = torch.zeros(1).to(device)\n",
        "    \n",
        "    optimizer.zero_grad()  # i. Explicitly set the gradients to zero\n",
        "    \n",
        "    z = 1 / (1 + torch.exp(torch.matmul(torch.cat([const_ones, X_train], dim=1), W1)))\n",
        "    z = 1 / (1 + torch.exp(torch.matmul(torch.cat([const_ones, z], dim=1), W2)))\n",
        "    z = torch.clamp(z, min=1e-6, max=1-1e-6)\n",
        "    z = -torch.sum((y_train * torch.log(z)) + ((1 - y_train) * torch.log(1 - z)))\n",
        "    with torch.no_grad():\n",
        "        cost = z.clone()\n",
        "    \n",
        "    z.backward()  # ii. Compute the gradients\n",
        "    \n",
        "    optimizer.step()  # iii. Update the parameters using the computed gradients \n",
        "\n",
        "    if n_epoch % 1000 == 0 or (n_epoch+1)==max_epochs:\n",
        "        print('epoch #'+str(n_epoch)+':', cost.item())\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0: 44920.55078125\n",
            "epoch #1000: 399.1661376953125\n",
            "epoch #2000: 269.6180419921875\n",
            "epoch #3000: 206.77101135253906\n",
            "epoch #4000: 167.23831176757812\n",
            "epoch #5000: 139.4957275390625\n",
            "epoch #6000: 118.78943634033203\n",
            "epoch #7000: 102.71863555908203\n",
            "epoch #8000: 89.90544128417969\n",
            "epoch #9000: 79.48570251464844\n",
            "epoch #9999: 70.889892578125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n1lQHeYRW6ma",
        "outputId": "2167d73c-ac74-44e7-993d-444ea24da733"
      },
      "source": [
        "# Compute training and test accuracy\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = 1 / (1 + torch.exp(torch.matmul(torch.cat([const_ones, X_train], dim=1), W1)))\n",
        "    z = 1 / (1 + torch.exp(torch.matmul(torch.cat([const_ones, z], dim=1), W2)))\n",
        "    y_train_pred = torch.argmax(z, 1)\n",
        "    y_train_true = torch.argmax(y_train, 1)\n",
        "\n",
        "    X_test = X_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    z = 1 / (1 + torch.exp(torch.matmul(torch.cat([torch.ones(X_test.shape[0], 1).to(device), X_test], dim=1), W1)))\n",
        "    z = 1 / (1 + torch.exp(torch.matmul(torch.cat([torch.ones(X_test.shape[0], 1).to(device), z], dim=1), W2)))\n",
        "    y_test_pred = torch.argmax(z, 1)\n",
        "    y_test_true = torch.argmax(y_test, 1)\n",
        "\n",
        "\n",
        "print('Training Accuracy =', (y_train_true == y_train_pred).sum().item() * 100 / y_train_true.shape[0],'\\b%')\n",
        "print('Test Accuracy =', (y_test_true == y_test_pred).sum().item() * 100 / y_test_true.shape[0],'\\b%')\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy = 99.92044550517105 \b%\n",
            "Test Accuracy = 97.5925925925926 \b%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU9URUvNYj9z"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 3. (ii) Using torch.nn to define a network, and use a predefined loss </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFfhsapGYiSy",
        "outputId": "03425237-939e-433e-f683-bde0fc45b37c"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layer_size, n_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.W1 = nn.Parameter(torch.rand(input_dim + 1, hidden_layer_size) * 2 - 1)\n",
        "        self.W2 = nn.Parameter(torch.rand(hidden_layer_size + 1, n_classes) * 2 - 1)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        const_ones = torch.ones(X.shape[-2], 1)\n",
        "        z = 1 / (1 + torch.exp(torch.matmul(torch.cat([const_ones, X], dim=1), self.W1)))\n",
        "        z = 1 / (1 + torch.exp(torch.matmul(torch.cat([const_ones, z], dim=1), self.W2)))\n",
        "        z = torch.clamp(z, min=1e-6, max=1-1e-6)\n",
        "        return z\n",
        "\n",
        "\n",
        "# Load in the digits data set from scikit-learn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "X = load_digits().data\n",
        "X = X / X.max()\n",
        "y = LabelBinarizer().fit_transform(load_digits().target) # Convert each label to one-hot vectors\n",
        "# Split the data into training and test data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
        "# Convert numpy arrays to torch tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "\n",
        "# Initialize network parameters\n",
        "input_dim = X.shape[1]\n",
        "hidden_layer_size = 200\n",
        "n_classes = y.shape[1]\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "mlp = MLP(input_dim=input_dim, hidden_layer_size=hidden_layer_size, n_classes=n_classes)\n",
        "\n",
        "\n",
        "# Push everything on to the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "mlp.to(device)\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "max_epochs = 10000\n",
        "learning_rate = 1e-4\n",
        "\n",
        "loss = nn.BCELoss(reduction='sum')\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "for n_epoch in range(max_epochs):\n",
        "    cost = torch.zeros(1).to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    z = mlp(X_train)\n",
        "    z = loss(z, y_train)\n",
        "    with torch.no_grad():\n",
        "        cost = z.clone()\n",
        "    \n",
        "    z.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    if n_epoch % 1000 == 0 or (n_epoch+1)==max_epochs:\n",
        "        print('epoch #'+str(n_epoch)+':', cost.item())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0: 32940.52734375\n",
            "epoch #1000: 411.0254821777344\n",
            "epoch #2000: 277.697265625\n",
            "epoch #3000: 213.82894897460938\n",
            "epoch #4000: 173.45681762695312\n",
            "epoch #5000: 144.90814208984375\n",
            "epoch #6000: 123.4655990600586\n",
            "epoch #7000: 106.75452423095703\n",
            "epoch #8000: 93.408447265625\n",
            "epoch #9000: 82.55364990234375\n",
            "epoch #9999: 73.60090637207031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIAAjvcnas2a",
        "outputId": "f33b2e2a-e961-4c75-f39f-9f22c9cb2fbf"
      },
      "source": [
        "with torch.no_grad():\n",
        "    z = mlp(X_train)\n",
        "    y_train_pred = torch.argmax(z, 1)\n",
        "    y_train_true = torch.argmax(y_train, 1)\n",
        "\n",
        "    X_test = X_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    z = mlp(X_test)\n",
        "    y_test_pred = torch.argmax(z, 1)\n",
        "    y_test_true = torch.argmax(y_test, 1)\n",
        "\n",
        "\n",
        "print('Training Accuracy =', (y_train_true == y_train_pred).sum().item() * 100 / y_train_true.shape[0],'\\b%')\n",
        "print('Test Accuracy =', (y_test_true == y_test_pred).sum().item() * 100 / y_test_true.shape[0],'\\b%')\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy = 99.84089101034209 \b%\n",
            "Test Accuracy = 97.4074074074074 \b%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzW3p4nrb1P_"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 3. (iii) Using layers defined in torch.nn </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOhE0ua-cLUd",
        "outputId": "38907b38-3522-4618-df60-26cafd889151"
      },
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_layer_size, n_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_layer_size)\n",
        "        self.fc2 = nn.Linear(hidden_layer_size, n_classes)\n",
        "    \n",
        "    def forward(self, X):\n",
        "        z = torch.sigmoid(self.fc1(X))\n",
        "        z = self.fc2(z)\n",
        "        return z\n",
        "\n",
        "\n",
        "# Load in the digits data set from scikit-learn\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import numpy as np\n",
        "X = load_digits().data\n",
        "X = X / X.max()\n",
        "y = LabelBinarizer().fit_transform(load_digits().target) # Convert each label to one-hot vectors\n",
        "# Split the data into training and test data sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=True)\n",
        "# Convert numpy arrays to torch tensors\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "\n",
        "# Initialize network parameters\n",
        "input_dim = X.shape[1]\n",
        "hidden_layer_size = 200\n",
        "n_classes = y.shape[1]\n",
        "\n",
        "\n",
        "# Initialize network\n",
        "mlp = MLP(input_dim=input_dim, hidden_layer_size=hidden_layer_size, n_classes=n_classes)\n",
        "\n",
        "\n",
        "# Push everything on to the GPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "X_train = X_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "mlp.to(device)\n",
        "\n",
        "\n",
        "# Training parameters\n",
        "max_epochs = 10000\n",
        "learning_rate = 1e-4\n",
        "\n",
        "loss = nn.BCEWithLogitsLoss(reduction='sum')\n",
        "optimizer = optim.SGD(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "for n_epoch in range(max_epochs):\n",
        "    cost = torch.zeros(1).to(device)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    z = mlp(X_train)\n",
        "    z = loss(z, y_train)\n",
        "    with torch.no_grad():\n",
        "        cost = z.clone()\n",
        "    \n",
        "    z.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    if n_epoch % 1000 == 0 or (n_epoch+1)==max_epochs:\n",
        "        print('epoch #'+str(n_epoch)+':', cost.item())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch #0: 8958.8310546875\n",
            "epoch #1000: 904.8087158203125\n",
            "epoch #2000: 527.9207763671875\n",
            "epoch #3000: 409.3244323730469\n",
            "epoch #4000: 342.25213623046875\n",
            "epoch #5000: 295.15966796875\n",
            "epoch #6000: 258.5216979980469\n",
            "epoch #7000: 228.5030059814453\n",
            "epoch #8000: 203.29234313964844\n",
            "epoch #9000: 181.77488708496094\n",
            "epoch #9999: 163.18670654296875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mqC42-u3eTjV",
        "outputId": "9009afec-1cf1-4e6e-bb7d-6222c1d5e622"
      },
      "source": [
        "with torch.no_grad():\n",
        "    z = mlp(X_train)\n",
        "    y_train_pred = torch.argmax(z, 1)\n",
        "    y_train_true = torch.argmax(y_train, 1)\n",
        "\n",
        "    X_test = X_test.to(device)\n",
        "    y_test = y_test.to(device)\n",
        "    z = mlp(X_test)\n",
        "    y_test_pred = torch.argmax(z, 1)\n",
        "    y_test_true = torch.argmax(y_test, 1)\n",
        "\n",
        "\n",
        "print('Training Accuracy =', (y_train_true == y_train_pred).sum().item() * 100 / y_train_true.shape[0],'\\b%')\n",
        "print('Test Accuracy =', (y_test_true == y_test_pred).sum().item() * 100 / y_test_true.shape[0],'\\b%')\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Accuracy = 99.28400954653938 \b%\n",
            "Test Accuracy = 97.5925925925926 \b%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx2UkLKAe7XJ"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 4. Using layers defined in torch.nn, and activation functions in torch.nn.functional </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCIGN16VfLY-",
        "outputId": "75c4c87a-30a2-435e-fdef-e6f0a5ee314e"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "a1 = torch.rand(4, 1, 28, 28)\n",
        "print('a1.shape', a1.shape)\n",
        "\n",
        "a2 = F.relu(nn.Conv2d(1, 6, 3)(a1))\n",
        "print('a2.shape', a2.shape)\n",
        "a3 = nn.MaxPool2d(2, 2)(a2)\n",
        "print('a3.shape', a3.shape)\n",
        "\n",
        "a4 = F.relu(nn.Conv2d(6, 16, 3)(a3))\n",
        "print('a4shape', a4.shape)\n",
        "a5 = nn.MaxPool2d(2, 2)(a4)\n",
        "print('a5.shape', a5.shape)\n",
        "\n",
        "a6 = a5.view(-1, 16 * 5 * 5)\n",
        "print('a6.shape', a6.shape)\n",
        "\n",
        "a7 = nn.Linear(16 * 5 * 5, 128)(a6)\n",
        "print('a7.shape', a7.shape)\n",
        "\n",
        "a8 = nn.Linear(128, 10)(a7)\n",
        "print('a8.shape', a8.shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a1.shape torch.Size([4, 1, 28, 28])\n",
            "a2.shape torch.Size([4, 6, 26, 26])\n",
            "a3.shape torch.Size([4, 6, 13, 13])\n",
            "a4shape torch.Size([4, 16, 11, 11])\n",
            "a5.shape torch.Size([4, 16, 5, 5])\n",
            "a6.shape torch.Size([4, 400])\n",
            "a7.shape torch.Size([4, 128])\n",
            "a8.shape torch.Size([4, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLBm9rF7fUjf"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 5. Using data sets present in torchvision </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxN2mi3yfh7Z",
        "outputId": "9c025ab5-f510-44ec-c944-224b04dfc684"
      },
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "print('Training data size:', trainset.data.shape)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "net = Net()\n",
        "net.to(device)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "max_epochs = 100\n",
        "for epoch in range(max_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        \n",
        "        optimizer.zero_grad()  # zero the parameter gradients\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "    if (epoch+1) % 10 == 0:\n",
        "        print('[epoch %d] loss: %.3f' %(epoch + 1, running_loss))\n",
        "print('Finished Training')\n",
        "\n",
        "\n",
        "PATH = './mnist_net.pth'\n",
        "torch.save(net.state_dict(), PATH)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training data size: torch.Size([60000, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[epoch 10] loss: 55.086\n",
            "[epoch 20] loss: 28.516\n",
            "[epoch 30] loss: 20.077\n",
            "[epoch 40] loss: 15.824\n",
            "[epoch 50] loss: 13.531\n",
            "[epoch 60] loss: 11.561\n",
            "[epoch 70] loss: 10.326\n",
            "[epoch 80] loss: 9.081\n",
            "[epoch 90] loss: 8.197\n",
            "[epoch 100] loss: 7.455\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMMDl-B5gaR1",
        "outputId": "f269e97d-8dae-42ee-b7a6-f76e684d3ed8"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "PATH = './mnist_net.pth'\n",
        "net = Net()\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "net.to(device)\n",
        "\n",
        "\n",
        "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "print('Test data size:', testset.data.shape)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = net(images)\n",
        "        predicted = torch.argmax(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        \n",
        "print('Accuracy =', 100 * correct / total)\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test data size: torch.Size([10000, 28, 28])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 98.53\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9cKjPxdOQMm"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 6. To fine-tune a pre-trained CNN, it’s enough to switch the \"requires_grad\" flags </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJZd3gM_OV_h"
      },
      "source": [
        "import torchvision\n",
        "\n",
        "model = torchvision.models.resnet18(pretrained=True)  # Load the ResNet-18 model with pre-trained layer weights\n",
        "\n",
        "# Prevent training of existing layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last fully-connected layer\n",
        "model.fc = nn.Linear(512, 100)  # Parameters of newly constructed modules have requires_grad=True by default\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Optimize only the classifier\n",
        "optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKq278QAiWLE"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> Recap:\n",
        "\n",
        "<h2> 1. torch.optim updates the parameters based on the computed gradients. </h2>\n",
        "\n",
        "<h2> 2. Steps of using an optimizer: </h2>\n",
        "<h2> (a) Initialize Optimizer </h2>\n",
        "<h2> (b) In the Optimization loop: </h2>\n",
        "<h2> i. Explicitly set the gradients to zero </h2>\n",
        "<h2> ii. Compute the gradients </h2>\n",
        "<h2> iii. Update the parameters using the computed gradients  </h2>\n",
        "\n",
        "<h2> 3. (i) Implementing a simple one-hidden layer Multi-Layered Perceptron  </h2>\n",
        "<h2> (ii) Using torch.nn to define a network, and use a predefined loss  </h2>\n",
        "<h2> (iii) Using layers defined in torch.nn  </h2>\n",
        "\n",
        "<h2> 4. Using layers defined in torch.nn, and activation functions in torch.nn.functional  </h2>\n",
        "\n",
        "<h2> 5. Using data sets present in torchvision  </h2>\n",
        "\n",
        "<h2> 6. To fine-tune a pre-trained CNN, it’s enough to switch the \"requires_grad\" flags </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> References at the PyTorch Documentation:\n",
        "\n",
        "<ul>\n",
        "    <br>\n",
        "    <li> <h2> <a href=\"https://pytorch.org/docs/stable/optim.html\"> torch.optim </a> </h2> </li>\n",
        "    <li> <h2> <a href=\"https://pytorch.org/docs/stable/nn.html\"> torch.nn </a> </h2> </li>\n",
        "    <li> <h2> <a href=\"https://pytorch.org/docs/stable/nn.functional.html\"> torch.nn.functional </a> </h2> </li>\n",
        "    <li> <h2> <a href=\"https://pytorch.org/vision/stable/datasets.html\"> torchvision.datasets </a> </h2> </li>\n",
        "    <li> <h2> <a href=\"https://pytorch.org/vision/stable/transforms.html\"> torchvision.transforms </a> </h2> </li>\n",
        "    <li> <h2> <a href=\"https://pytorch.org/vision/stable/models.html\"> torchvision.models </a> </h2> </li>\n",
        "</ul>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    }
  ]
}