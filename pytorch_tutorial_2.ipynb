{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch_tutorial_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIQQ2tvgLSDP"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?id=1EOzaynFEkpMjE-PWPqllAYWRScW-2YQ0\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXa12UH8FWpz"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "\n",
        "<h1> Avisek Gupta </h1>\n",
        "\n",
        "<h1> Indian Statistical Institute, Kolkata </h1>\n",
        "\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4vT5CE9N1D"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<center>\n",
        "\n",
        "<h1> Tutorial 2: Tensor Gradients and Optimization </h1>\n",
        "\n",
        "</center>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9IEMzohDOPa"
      },
      "source": [
        "import torch"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTzXfxKvGpiv"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 1. Computing Tensor Gradients : </h2>\n",
        "\n",
        "<h2> (i) Every Tensor has a flag ''requires_grad'' that allows for fine grained exclusion of subgraphs from gradient computation </h2>\n",
        "\n",
        "<h2> (ii) By default, the gradients of user-created tensors are not calculated. </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfYTC-tuG6j_",
        "outputId": "6c9909c0-c791-403e-8d34-8779e1492be7"
      },
      "source": [
        "x = torch.ones(2, 2)\n",
        "print(x)\n",
        "print(x.requires_grad)\n",
        "\n",
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "print(x.requires_grad)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "False\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9P314nqBHV8f"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> (iii) The backward() function calculates the gradients. </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geiiSq7lHbYl",
        "outputId": "3491dced-d1d0-4116-cc11-ad61296ba970"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = 3 * (y ** 2)\n",
        "print(z)\n",
        "\n",
        "out = z.mean()\n",
        "print(out)\n",
        "\n",
        "out.backward()  # equivalent to out.backward(torch.tensor(1.)) since out contains a single scalar\n",
        "\n",
        "print(x.grad)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpRKP0z1Iw4R"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> (iv) We cannot print the gradients of intermediate tensors y or z, even though they are calculated. </h2>\n",
        "\n",
        "<h2> (v) Only if ALL inputs tensors DO NOT require gradients, the gradient of the output tensor will NOT be tracked. </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-cCoapMIVsO",
        "outputId": "8ac9927b-5301-4b56-9ebd-d4a430a3c4f6"
      },
      "source": [
        "print('y:', y)\n",
        "print('y.grad:', y.grad)\n",
        "print('y.requires_grad:', y.requires_grad)\n",
        "print('y.grad_fn:', y.grad_fn)\n",
        "\n",
        "print('z:', z)\n",
        "print('z.grad:', z.grad)\n",
        "print('z.requires_grad:', z.requires_grad)\n",
        "print('z.grad_fn:', z.grad_fn)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y: tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "y.grad: None\n",
            "y.requires_grad: True\n",
            "y.grad_fn: <AddBackward0 object at 0x7f0d90ac2e10>\n",
            "z: tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "z.grad: None\n",
            "z.requires_grad: True\n",
            "z.grad_fn: <MulBackward0 object at 0x7f0d90ad51d0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsGa5F4PJ-V7"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 2. Stop automatic computation of gradients on Tensors with \"requires_grad=True\" either by - </h2>\n",
        "    \n",
        "<h2> (i) Wrapping the code block in with torch.no_grad(), or </h2>\n",
        "    \n",
        "<h2> (ii) By using .detach() to get a new Tensor with the same content but that does not require gradients </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZbm7xoTI-Dv",
        "outputId": "e4cad7c0-7313-4f86-bf9f-efe974251cb4"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)\n",
        "    "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DuUTpi3LKol",
        "outputId": "dd84f6a2-849b-4ac3-82c6-6c1ec5248ff8"
      },
      "source": [
        "print(x.requires_grad)\n",
        "y = x.detach()\n",
        "print(y.requires_grad)\n",
        "print(x.eq(y).all())\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n",
            "tensor(True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk3D39EBMw_K"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> 3. A simple gradient descent code using only backward() to compute the gradients </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cR6UywaFNDGi",
        "outputId": "2ca79eb2-c79d-4004-80ee-9737841a5c98"
      },
      "source": [
        "# First lets try to code only a single step of gradient descent\n",
        "\n",
        "x = torch.randn(1, requires_grad=True)\n",
        "print('x:', x)\n",
        "\n",
        "y = (x - 4) ** 2\n",
        "y.backward()\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "print('x.grad:', x.grad)\n",
        "print('x - x.grad:', x - learning_rate * x.grad)\n",
        "with torch.no_grad():\n",
        "    x = x - learning_rate * x.grad\n",
        "print('x:', x)\n",
        "x.requires_grad = True\n",
        "print('x:', x)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x: tensor([0.2121], requires_grad=True)\n",
            "x.grad: tensor([-7.5758])\n",
            "x - x.grad: tensor([0.9697], grad_fn=<SubBackward0>)\n",
            "x: tensor([0.9697])\n",
            "x: tensor([0.9697], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rni26Z5MNIzU",
        "outputId": "d0d13655-66d1-434b-8562-b3b4f80ad623"
      },
      "source": [
        "# Gradient Descent till convergence\n",
        "\n",
        "x = torch.randn(1, requires_grad=True)\n",
        "print('Initial x:', x)\n",
        "\n",
        "max_iter = 100\n",
        "learning_rate = 0.1\n",
        "eps = 1e-6\n",
        "\n",
        "for _ in range(max_iter):\n",
        "    y = (x - 4) ** 2\n",
        "    y.backward()\n",
        "    with torch.no_grad():\n",
        "        prev_x = x\n",
        "        x = x - learning_rate * x.grad\n",
        "        if torch.norm(prev_x - x) < eps:\n",
        "            break\n",
        "    x.requires_grad = True\n",
        "\n",
        "print('Soln x:', x)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial x: tensor([0.7421], requires_grad=True)\n",
            "Soln x: tensor([4.0000])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlLlKfOsNam3"
      },
      "source": [
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h2> Recap: </h2>\n",
        "\n",
        "<h2> 1. Computing Tensor Gradients : </h2>\n",
        "\n",
        "<h2> (i) Every Tensor has a flag ''requires_grad'' that allows for fine grained exclusion of subgraphs from gradient computation </h2>\n",
        "\n",
        "<h2> (ii) By default, the gradients of user-created tensors are not calculated. </h2>\n",
        "\n",
        "<h2> (iii) The backward() function calculates the gradients. </h2>\n",
        "\n",
        "<h2> (iv) We cannot print the gradients of intermediate tensors y or z, even though they are calculated. </h2>\n",
        "\n",
        "<h2> (v) Only if ALL inputs tensors DO NOT require gradients, the gradient of the output tensor will NOT be tracked. </h2>\n",
        "\n",
        "<h2> 2. Stop automatic computation of gradients on Tensors with \"requires_grad=True\" either by - </h2>\n",
        "    \n",
        "<h2> (i) Wrapping the code block in with torch.no_grad(), or </h2>\n",
        "    \n",
        "<h2> (ii) By using .detach() to get a new Tensor with the same content but that does not require gradients </h2>\n",
        "\n",
        "<h2> 3. A simple gradient descent code using only backward() to compute the gradients </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<hr>\n",
        "\n",
        "<br>\n"
      ]
    }
  ]
}